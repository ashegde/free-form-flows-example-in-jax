{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In this notebook, we consider a simple example of normalizing flows, based on the free-form formulation in, \n",
    "\n",
    "Draxler, F., Sorrenson, P., Zimmermann, L., Rousselot, A., & KÃ¶the, U. (2024, April). \n",
    "Free-form flows: Make any architecture a normalizing flow. \n",
    "In International Conference on Artificial Intelligence and Statistics (pp. 2197-2205). PMLR.\n",
    "\n",
    "As documented in the reference, the advantage of this formulation is that it avoids the need to explicitly use\n",
    "invertible neural networks. Rather, this is enforced through a penalty that promotes reconstruction.\n",
    "\n",
    "In the following code, we will use only basic jax code as a learning exersize, akin to\n",
    "\n",
    "https://jax.readthedocs.io/en/latest/notebooks/neural_network_with_tfds_data.html\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from typing import Callable\n",
    "import matplotlib.pyplot as plt\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.devices()\n",
    "key = jax.random.key(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings\n",
    "NUMBER_OF_EPOCHS = 1200\n",
    "BATCH_SIZE = 256\n",
    "DATA_SIZE = 10000\n",
    "LEARNING_RATE = 1e-3\n",
    "BETA = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "\n",
    "def generate_dataset(n: int = 1000, r_inner = 0.5, r_outer = 1.5):\n",
    "    #theta = np.linspace(0, 2 * np.pi, n)\n",
    "    theta = np.random.rand(n) * 2 * np.pi\n",
    "    r = r_inner + (r_outer - r_inner) / 2 * (1 + np.sin(2 * theta))\n",
    "    noise = 0.1 * (2.0 * np.random.rand(n) - 1.0)\n",
    "    x = r * np.cos(theta) * (1 + noise)\n",
    "    y = r * np.sin(theta) * (1 + noise)\n",
    "\n",
    "    return np.stack([x, y], axis=1)\n",
    "\n",
    "\n",
    "train_data = jnp.array( generate_dataset(n=DATA_SIZE) )\n",
    "valid_data = jnp.array( generate_dataset(n=100) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(train_data[:,0], train_data[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def random_layer_params(\n",
    "#         key: jax.random.key,\n",
    "#         in_dim: int,\n",
    "#         out_dim: int,\n",
    "#         b_scale: float = 0.0,\n",
    "# ) -> tuple[jnp.array]:\n",
    "    \n",
    "#     w_key, b_key = jax.random.split(key, 2)\n",
    "#     return  (\n",
    "#       1/jnp.sqrt(in_dim) * jax.random.uniform(\n",
    "#            w_key,\n",
    "#            shape=(in_dim, out_dim),\n",
    "#            minval=-1.0,\n",
    "#            maxval=1.0,\n",
    "#         ),\n",
    "#       b_scale * jax.random.uniform(\n",
    "#            b_key,\n",
    "#            shape=(out_dim,),\n",
    "#            minval=-1.0,\n",
    "#            maxval=1.0,\n",
    "#         ),\n",
    "#     )\n",
    "\n",
    "def random_layer_params(\n",
    "        key: jax.random.key,\n",
    "        in_dim: int,\n",
    "        out_dim: int,\n",
    "        b_scale: float = 0.0,\n",
    ") -> tuple[jnp.array]:\n",
    "    \n",
    "    w_key, b_key = jax.random.split(key, 2)\n",
    "    return  (\n",
    "      jnp.sqrt(2/(in_dim)) * jax.random.normal(\n",
    "           w_key,\n",
    "           shape=(in_dim, out_dim),\n",
    "        ),\n",
    "      b_scale * jax.random.normal(\n",
    "           b_key,\n",
    "           shape=(out_dim,),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "# Initialize all layers for a fully-connected neural network with sizes \"sizes\"\n",
    "def init_network_params(key: jax.random.key, layer_sizes: list[int], b_scale: int = 0.0) -> list[int]:\n",
    "    keys = jax.random.split(key, len(layer_sizes))\n",
    "    return [\n",
    "        random_layer_params(k, in_dim, out_dim, b_scale) \n",
    "        for in_dim, out_dim, k in zip(layer_sizes[:-1], layer_sizes[1:], keys)\n",
    "    ]\n",
    "\n",
    "layer_sizes = [2, 32, 64, 64, 32, 2]\n",
    "b_scale = 1e-3\n",
    "\n",
    "key, subkey1, subkey2 = jax.random.split(key, 3)\n",
    "encoder_params = init_network_params(subkey1, layer_sizes, b_scale)\n",
    "decoder_params = init_network_params(subkey2, layer_sizes, b_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model prediction\n",
    "# empirically, including layer-wise residual connections (e.g., h = h + activation(h)) results in the loss\n",
    "# blowing up, leading to nans. The trainability (with just SGD) of this loss is quite sensitive to the network characteristics\n",
    "# and the network/loss hyperparameters.\n",
    "def predict_single(params: jnp.array, x: jnp.array) -> jnp.array:\n",
    "    def relu(z: jnp.array):\n",
    "        return jnp.maximum(0, z)\n",
    "    def tanh(z: jnp.array):\n",
    "        return jnp.tanh(z)\n",
    "    def sigmoid(z: jnp.array):\n",
    "        return 1/(1 + jnp.exp(-z))\n",
    "    def selu(z: jnp.array):\n",
    "        return z * sigmoid(z)\n",
    "    \n",
    "    h = x\n",
    "    for (w,b) in params[:-1]:\n",
    "        h = jnp.dot(h,w) + b\n",
    "        # h = relu(h)  \n",
    "        h = tanh(h)\n",
    "    \n",
    "    final_w, final_b = params[-1]\n",
    "    return x + jnp.dot(h, final_w) + final_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss\n",
    "\n",
    "def fff_components_single(\n",
    "    key:  jax.random.key,\n",
    "    encoder_params: list[jnp.array],\n",
    "    decoder_params: list[jnp.array],\n",
    "    x: jnp.array,\n",
    "):\n",
    "    \"\"\"\n",
    "    Free-form flow (fff) loss function, as defined in Algorithm 1 of the\n",
    "    primary reference and derived in Appendix A.2. \n",
    "    \"\"\"\n",
    "\n",
    "    # note, x is assumed of dimension (d,) \n",
    "    # also, jacobian calculations are wrt inputs (not parameters)\n",
    "    \n",
    "    def encoder_fn(x1: jnp.array):\n",
    "       return predict_single(encoder_params, x1)\n",
    "    \n",
    "    def decoder_fn(z1: jnp.array):\n",
    "        return predict_single(decoder_params, z1)\n",
    "    \n",
    "    # Hutchinson trace approximation works better with test vectors v on \\sqrt{dim}-sphere.\n",
    "    # https://www.ethanepperly.com/index.php/2024/01/28/dont-use-gaussians-in-stochastic-trace-estimation/\n",
    "    #\n",
    "    # following the main reference, we will just use a single test vector\n",
    "\n",
    "    \n",
    "    v = jax.random.normal(key, shape=x.shape)\n",
    "    v *= jnp.sqrt(v.shape[-1]) / jnp.sqrt(jnp.square(v).sum(axis=-1, keepdims=True))\n",
    "    \n",
    "    z, func_vjp = jax.vjp(encoder_fn, x)\n",
    "    v1 = func_vjp(v)[0]\n",
    "    xr, v2 = jax.jvp(decoder_fn, [z,], [v,])\n",
    " \n",
    "    \n",
    "    # v, z, v1, xr, v2 are all lists containing a (B, 1, d) array\n",
    "    log_jac_det = jax.lax.stop_gradient(v2) * v1\n",
    "    nll = 0.5 * jnp.square(z).sum(axis=-1) - log_jac_det.sum(axis=-1)\n",
    "    L_reconstr = jnp.square(xr - x).sum(axis=-1)\n",
    "\n",
    "    return nll, L_reconstr\n",
    "\n",
    "\n",
    "# define vmap over rng keys and batches\n",
    "batch_fff_components = jax.vmap(fff_components_single, in_axes=(0, None, None, 0))\n",
    "\n",
    "def batch_loss(\n",
    "    key:  jax.random.key,\n",
    "    encoder_params: list[jnp.array],\n",
    "    decoder_params: list[jnp.array],\n",
    "    x: jnp.array,\n",
    "):\n",
    "    keys = jax.random.split(key, x.shape[0])\n",
    "    nll, L_reconstr = batch_fff_components(keys, encoder_params, decoder_params, x)\n",
    "    return (nll + BETA * L_reconstr).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class AdamW():\n",
    "#     def __init__(self)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def update(\n",
    "    key:  jax.random.key,\n",
    "    encoder_params: list[jnp.array],\n",
    "    decoder_params: list[jnp.array],\n",
    "    x: jnp.array,\n",
    "    lr: float,\n",
    ")->tuple[list]:\n",
    "    grad_fn = jax.grad(batch_loss, argnums=(1, 2))\n",
    "    encoder_grads, decoder_grads = grad_fn(key, encoder_params, decoder_params, x)\n",
    "\n",
    "    return (\n",
    "        [(w - lr * dw, b - lr * db) for (w, b), (dw, db) in zip(encoder_params, encoder_grads)],\n",
    "        [(w - lr * dw, b - lr * db) for (w, b), (dw, db) in zip(decoder_params, decoder_grads)],\n",
    "    )\n",
    "\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "\n",
    "def batch_iterate(key: jax.random.key, batch_size: int, x: jnp.array):\n",
    "    perm = jnp.array(jax.random.permutation(key, x.shape[0]))\n",
    "    for s in range(0, x.shape[0], batch_size):\n",
    "        ids = perm[s : s + batch_size]\n",
    "        yield x[ids]\n",
    "\n",
    "losses = []\n",
    "lr = LEARNING_RATE\n",
    "scale = 0.99\n",
    "for e in range(NUMBER_OF_EPOCHS):\n",
    "    key, subkey1 = jax.random.split(key, 2)\n",
    "    start_time = time.time()\n",
    "    for xb in batch_iterate(subkey1, BATCH_SIZE, train_data):\n",
    "        key, subkey2 = jax.random.split(key, 2)\n",
    "        encoder_params, decoder_params = update(\n",
    "            subkey2,\n",
    "            encoder_params,\n",
    "            decoder_params,\n",
    "            xb,\n",
    "            lr,\n",
    "        )\n",
    "\n",
    "    lr *= scale\n",
    "    subkeys = jax.random.split(key, train_data.shape[0]+1)\n",
    "    key = subkeys[0]\n",
    "    train_nll, train_reconstr = batch_fff_components(\n",
    "        subkeys[1:],\n",
    "        encoder_params,\n",
    "        decoder_params,\n",
    "        train_data,\n",
    "    )\n",
    "    train_nll = train_nll.mean()\n",
    "    train_reconstr = train_reconstr.mean()\n",
    "    train_loss = (train_nll + BETA * train_reconstr).mean()\n",
    "\n",
    "    subkeys = jax.random.split(key, valid_data.shape[0]+1)\n",
    "    key = subkeys[0]\n",
    "    valid_nll, valid_reconstr = batch_fff_components(\n",
    "        subkeys[1:],\n",
    "        encoder_params,\n",
    "        decoder_params,\n",
    "        valid_data,\n",
    "    )\n",
    "    valid_nll = valid_nll.mean()\n",
    "    valid_reconstr = valid_reconstr.mean()\n",
    "    valid_loss = (valid_nll + BETA * valid_reconstr).mean()\n",
    "        \n",
    "        \n",
    "    epoch_time = time.time() - start_time\n",
    "    print(f\"Epoch {e} | lr: {lr:.2e} | Train nll, reconstr, loss: {train_nll.item():.3f}, {train_reconstr.item():.3f}, {train_loss.item():.3f} | Valid loss {valid_loss.item():.3f} | Epoch time {epoch_time:.4f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_predict = jax.vmap(predict_single, in_axes=(None, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generation plot\n",
    "num_samples = 1000\n",
    "key, subkey = jax.random.split(key, 2)\n",
    "z = jax.random.normal(subkey, shape=(num_samples, 2))\n",
    "x_gen = batch_predict(decoder_params, z)\n",
    "\n",
    "plt.scatter(x_gen[:, 0], x_gen[:, 1], c='blue', marker='.', label='decoded/generated')\n",
    "plt.scatter(z[:, 0], z[:, 1], marker='o', facecolors='none', edgecolors='black', alpha=0.2, label='latents')\n",
    "plt.scatter(train_data[:, 0], train_data[:, 1], c='red', marker='.', alpha = 1e-2, label='data')\n",
    "leg = plt.legend()\n",
    "for lhandle in leg.legend_handles: \n",
    "    lhandle.set_alpha(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax-metal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
