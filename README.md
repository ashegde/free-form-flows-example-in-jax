# free-form-flows-example-in-jax

In the above code, we take a look at a simple 2D example of the free-form normalizing flows method, which was developed in

Draxler, F., Sorrenson, P., Zimmermann, L., Rousselot, A., & KÃ¶the, U. (2024, April). 
Free-form flows: Make any architecture a normalizing flow. 
In International Conference on Artificial Intelligence and Statistics (pp. 2197-2205). PMLR.

As documented in the reference, the advantage of this formulation is that it avoids the need to explicitly use
invertible neural networks. Rather, "invertibility" is enforced through a reconstruction penalty.

The code here is written in fairly plain JAX and utilizes a simple MLP, which is trained using just SGD. 
Despite this, we are able to generate non-trivial results, albeit after substantial hyper-parameter tuning.
For many choices of network settings and hyperparameters, the training process is rather unstable.

The results can be regenerated by running,

```
python run_example.py
```

which produces the following result:

![generated_samples](https://github.com/user-attachments/assets/64a96a7e-cea4-4d66-ba08-86779f2e1cb1)
